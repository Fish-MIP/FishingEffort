---
editor_options: 
  chunk_output_type: console
---

# Libraries

```{r}

rm(list= ls())

library(tidyverse)
library(dtplyr)
library(dplyr)
library(data.table)
library(vroom)
library(parallel)
library(tictoc)

# library(geoR)
library(raster)
library(here)
library(RColorBrewer)


select <- dplyr::select

yannick_dir <- "/rd/gem/private/users/yannickr"
original_effort_dir <- "/rd/gem/private/users/yannickr/effort_mapped_bycountry"

aggregated_files_dir <- "/rd/gem/private/users/yannickr/effort_mapped_by_country_aggregated_regional_models/"

```

# Explore mask

```{r}

(Model_region <- readRDS(file.path(yannick_dir, "Cells_LatLon_model_regions.rds")) %>% rename(Lon=lon, Lat = lat) %>% as.data.table())
# unique(Model_region$Lon)
Model_region <- lazy_dt(Model_region)

```

Make a function that will speedily import and merge the data files

```{r}

join_effort_data <- function(this_file_name){
  
  # # trial
  # original_effort_files <- list.files(file.path(original_effort_dir), pattern = ".csv")
  # this_file_name<-original_effort_files[[2]]
  # # end trial 
  
  this_source_path <- file.path("/rd/gem/private/users/yannickr/effort_mapped_bycountry", this_file_name)
  
  this_destination_path <- paste0("/rd/gem/private/users/yannickr/effort_mapped_by_country_aggregated_regional_models/", "aggregated_regional_model_", this_file_name)
  
  # if(file.exists(this_destination_path)){ # WARNING - CN why this?? files do not exist here. 
    
    Year <- as.numeric(str_extract(this_file_name, pattern =  "([[:digit:]])+"))
    
    these_data <- fread(this_source_path)
    these_data <- lazy_dt(these_data)
    
    # # WARNING - there is land data here - why?
    # # figure out land - I think it's the coarser resolution used by Ryan's map?
    # # use original_effort_files[[2]] for this
    # trial2 <- these_data  %>%
    #   left_join(Model_region, by=c("Lat", "Lon")) %>% 
    #   as.data.frame() %>% 
    #   # filter(model_area_name == "Land") %>% 
    #   select(Lat, Lon,NomActive, model_area_name) %>% 
    #   unique() 
    # # 9.75 98.25 # these are mostly coastal. 
    # # See if points in regional model are actually in this area
    # unique(trial2$model_area_name)
    # # original_effort_files[[110]]
    # # North sea 59.75 -1.25 (lat, lon) # OK corresponds to NS on google map
    # filter(trial2, model_area_name == "North_Sea")
    
    #dtplyr approach
    these_aggregated_data <-
      these_data %>%
      left_join(Model_region, by=c("Lat", "Lon")) %>% 
      group_by(model_area_name, SAUP, Gear, FGroup, Sector) %>%
      summarise(NomActive = sum(NomActive, na.rm = TRUE),
                EffActive = sum(EffActive, na.rm = TRUE),
                NV= sum(NV, na.rm = TRUE),
                P= sum(P, na.rm = TRUE),
                GT= sum(GT, na.rm = TRUE)) %>%
      mutate(Year = Year) %>% as.data.table()
    
    fwrite(x = these_aggregated_data, file = file.path(this_destination_path))
  # }
  
}


```

# Run the code on all 30,000 files and write them to folder

```{r}

#compressed_effort_files <- list.files(file.path(compressed_effort_dir), full.names = TRUE)
original_effort_files <- list.files(file.path(original_effort_dir), pattern = ".csv")

chunk_size <- 500 #chunk size for processing
effort_list_split <- split(original_effort_files, ceiling(seq_along(original_effort_files)/chunk_size))
length(effort_list_split) #60 chunks

# CN trial 
# effort_list_split<-effort_list_split[1:2]

tic()
for(i in 1:length(effort_list_split)){
  
  file_chunk <- effort_list_split[[i]]
  
  message("Processing chunk #", i, " of ", length(effort_list_split))
  
  mclapply(X = file_chunk, FUN = join_effort_data, mc.cores = 40)
  
}
toc()
3309.716/3600 # 1 hour to run

```

# Check the files

```{r}
#check the files
newly_written_files <- list.files(file.path(yannick_dir, "effort_mapped_by_country_aggregated_regional_models"), full.names = TRUE)

# pick one randomly 
map(newly_written_files[[8]], fread)

```

# Combine all listed files to one file

```{r}

combined_aggregated_effort <- rbindlist(mclapply(X = newly_written_files, FUN = fread, mc.cores = 40))
fwrite(x = combined_aggregated_effort, file.path(yannick_dir, "all_effort_aggregated_regional_models.csv"))

```

