---
title: "Catch_data"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The aim of this file is to aggregate Reg's historical catch data as per Yannick's effort data 

ORIGIN: data sent from Reg on 20/05/22 - RE: question about mapping catch and effort

WARNING on units and weights: Should we weight catch (and effort, script 02) by grip cell area when summarising at LME, EEZ level? it seems not - see Reg's email conversation. Also the non-weighted value match Yannick adn Reg's global totals. 
FORMAT:  
1. from Reg: Data by cell (with lat and long) by year by country fishing (represented by a code CNumber) by FGroup (represented by a code). The numbers are in tonnes for reported, IUU and Discards. Typically you would ignore discards as they can be any species (not the one in the record usually). So I would combine reported and IUU estimated.
2. Catch is in tonnes (by grid cell), can /area grid cell to get catch rates. Reg NOTES: "PPS if you decide to map using the cell data then you should NOT use the tonnes but rather the catch rate (tonnes per sq km of ocean) as the cells vary in size and tonnes would be misleading ..."

# Add LME and EEZ to historical cacth data from Reg 

```{r new data}

rm(list=ls())

library(tidyverse)
library(data.table)
library(dtplyr)
library(tictoc)

# historical/ industrial 
hist<-fread("/rd/gem/private/users/yannickr/Reg_catchData/20220606_RegCatchData/HistoricalIndCami.csv")
head(hist)
nrow(hist)
hist<-lazy_dt(hist)

# non historical / non industrial 
recent_art<-fread("/rd/gem/private/users/yannickr/Reg_catchData/20220606_RegCatchData/NonHistoricalNIndCami.csv")
head(recent_art)
nrow(recent_art)
recent_art<-lazy_dt(recent_art)

# non historical / industrial
tic()
recent_ind<-fread("/rd/gem/private/users/yannickr/Reg_catchData/20220606_RegCatchData/NonHistoricalIndCami_toCombine/NonHistoricalIndCami.csv")
head(recent_ind) 
nrow(recent_ind)
toc()
recent_ind<-lazy_dt(recent_ind)

# all data merged! 
catch<-hist %>% 
  full_join(recent_art) %>% 
  full_join(recent_ind)

# head(as.data.table(catch))
# nrow(as.data.table(catch)) # 116450350
# nrow(hist) + nrow(recent_ind) + nrow(recent_art) # 116450350
# sort(unique(as.data.table(catch)$Year)) # 1869 - 2017

# use same approach as in 02_merge_effort_spetial_data.Rmd to add LME, EEZ, FAO regions and aggregate data by these groups

### paste code from script 02 ----

library(dplyr)
library(vroom)
library(parallel)
library(geoR)
library(raster)
library(here)
library(RColorBrewer)

select <- dplyr::select

yannick_dir <- "/rd/gem/private/users/yannickr"
# original_effort_dir <- "/rd/gem/private/users/yannickr/effort_mapped_bycountry"
# aggregated_files_dir <- "/rd/gem/private/users/yannickr/effort_mapped_by_country_aggregated/"

# check and adjust EEZ file 
(EEZ <- read_csv(file.path(yannick_dir, "Cells_LatLon_EEZ.csv"))) 
#EEZ data is duplicated in places because of disputed claims to cells - just going to take the first country listed
(EEZ_disputes <- read_csv(file.path(yannick_dir, "Cells_LatLon_EEZ.csv")) %>% filter(DISPUTED==TRUE))
(take_first <- EEZ_disputes[!duplicated(EEZ_disputes[c("Lon", "Lat")]),])
(EEZ_adj <- bind_rows(EEZ %>% 
            filter(DISPUTED == FALSE),
            take_first) %>%
    select(Lat, Lon, A_Code, Admin_Country) %>% 
    dplyr::rename(eez_country_code = A_Code, 
           eez_country_name = Admin_Country) %>% 
    distinct())

united_coords <- EEZ_adj %>% unite("coords", c("Lon", "Lat"), sep = " ")

EEZ_adj <- EEZ_adj[-which(duplicated(united_coords[,"coords"])==TRUE),] %>% as.data.table() # remove duplicated EEZ records
EEZ_adj <- lazy_dt(EEZ_adj)

#check EEZ data - EEZs look ok
EEZ_xyz <- EEZ_adj %>% as_tibble() %>%  dplyr::rename(x=Lon, y= Lat, z=eez_country_code) %>%  select(x,y,z)  
EEZ_raster <- raster::rasterFromXYZ(EEZ_xyz)

# ggplot(data = EEZ_xyz)+
#   geom_tile(aes(x=x, y=y, fill=z)) + 
#   scale_fill_gradientn(colours = RColorBrewer::brewer.pal(n = 9, name = "Spectral"))

# LME data
(LMEs <- fread(file.path(yannick_dir, "LMESeq.csv")))
LMEs <- lazy_dt(LMEs)

#FAO regions data
(FAO_regions <- readRDS(file.path(yannick_dir, "Cells_LatLon_FAO_regions.rds")) %>% 
    dplyr::rename(Lon=x, Lat = y) %>% mutate(fao_area= as.double(fao_area)) %>% as.data.table())
FAO_regions <- lazy_dt(FAO_regions)

# Looking at the CNumber - this appears the be the iso3n code for the country. This is often the name the same as the admin country but is different when the land masses are territories of other countries

#The data have different rows. Want to make sure they line up
nrow(EEZ_adj) #179904
nrow(as_tibble(LMEs)) #259200
nrow(FAO_regions) #229369

#Some LME data are around that have no FAO or EEZ data
(all_spatial_layers <- 
  full_join(x = full_join(x = LMEs %>% select(-Seq), y = FAO_regions, by = c("Lon", "Lat")), 
            y = EEZ_adj %>% select(Lon, Lat, eez_country_code, eez_country_name), by = c("Lon", "Lat")) %>% as_tibble()
)

missing_fao_eez <- 
  all_spatial_layers %>% 
  filter(is.na(fao_area)) %>% #29831 where FAO areas are NA but LME are not
  filter(is.na(eez_country_code)) #29830 where EEZ are NA but LME are not - this does not change depending on order. So there is one cell where the is LME data and EEZ data but not FAO area data

## end paste ----

# add LME and  EEZ to the catch and aggregate ---- 

# unique(sort(as.data.table(catch)$Lat))
# unique(sort(as.data.table(catch)$Lon))
# 
# unique(sort(as.data.table(LMEs)$Lat))
# unique(sort(as.data.table(LMEs)$Lon))
# 
# unique(sort(as.data.table(EEZ_adj)$Lat))
# unique(sort(as.data.table(EEZ_adj)$Lon))

tic()
catch2 <- catch %>%
  select(-Seq) %>% 
  left_join(LMEs, by=c("Lat", "Lon")) %>%
  left_join(EEZ_adj, by=c("Lat", "Lon")) %>%
  # left_join(FAO_regions, by=c("Lat", "Lon")) %>%
  
  # WARNING: Should we weight by grid cell area here??? Same for effort (in script 02)! See comment top of file. 

  group_by(Year,CNumber,FGroup, LME, eez_country_code,eez_country_name,Sector) %>%
  summarise(Reported = sum(Reported, na.rm = TRUE),
            IUU = sum(IUU, na.rm = TRUE),
            Discards = sum(Discards, na.rm = TRUE)) %>% 
  ungroup() %>% 
  as.data.table() 
toc()

# # should we sum catches?
# catch2<-catch2 %>%
#   rowwise(Year,CNumber, FGroup,LME,eez_country_code,eez_country_name,Sector) %>%
#   dplyr::summarise(catch = sum(Reported,IUU)) %>% # dischards not usually accounted for
#   ungroup() # %>%
#   # mutate(Sector = "Industrial")

```

# format catch as effort file  

```{r catch}

# ORIGIN: created through script 02 from original Yannicks country files in Gem48 and re-arranged and trimmed in script 05.    
effort<-fread("/rd/gem/private/users/yannickr/effort_histsoc_1950_2017.csv")
head(effort)
effort<-effort[,-1]

# do the LME and EEZ match? ----

# # EEZ - OK
# sort(unique(catch$eez_country_name))
# sort(unique(effort$eez_country_name))
# 
# # LME - OK
# sort(unique(catch$LME)) 
# sort(unique(effort$LME))

# fishing country 
saup<-read_csv("/rd/gem/private/users/yannickr/SAUPcode_to_Country.csv")

# # is CNumber == SAUP? it looks like it
# sort(unique(catch$CNumber)) 
# sort(unique(saup$SAUP_Country_Nbr))
# unique(filter(catch, CNumber == 124)$eez_country_name)
# filter(saup, SAUP_Country_Nbr == 124)

saup<-saup %>% 
  dplyr::rename(SAUP = SAUP_Country_Nbr,
                FCountryName = Country) %>% 
  select(-Region)

catch2<-catch2 %>% 
  dplyr::rename(SAUP = CNumber) %>% 
  left_join(saup, by ="SAUP")

# FGroup
taxon<-read_csv("/rd/gem/private/users/yannickr/Reg_catchData/20220606_RegCatchData/CodesRevisedJune2022_FGroup.csv")

# clean
taxon<-taxon %>% 
  mutate(Description = gsub(" ", "", Description, fixed = TRUE),
         Description = gsub(",", "", Description, fixed = TRUE)) %>% # remove white spaces 
  select(FunctionalGrp, Description) %>% 
  unique()

# # any missing one in effort/ catch? No - all OK 
# setdiff(unique(taxon$Description), unique(effort$FGroup))
# setdiff(unique(effort$FGroup),unique(taxon$Description))
# 
# sort(unique(taxon$Description))
# sort(unique(effort$FGroup))

# merge 
catch2<-catch2 %>% 
  left_join(taxon %>% dplyr::rename(FGroup = FunctionalGrp)) %>% 
  select(-FGroup) %>% 
  dplyr::rename(FGroup = Description)

# sector
catch2<-catch2 %>% 
  mutate(Sector = ifelse(Sector == 2, "artisanal", "industrial"))

```

# check catch data given Reg summaries and plots

```{r}

annual<-read_csv("/rd/gem/private/users/yannickr/Reg_catchData/20220606_RegCatchData/CheckCatchTotalsCN.csv")

# unique(annual$Sector)
annual<-annual %>% 
  mutate(Sector = ifelse(Sector == "artisanal", "artisanal", "industrial")) %>% 
  select(-"...6") %>% 
  mutate(type = "summaries")

trial<-catch2 %>% 
  group_by(Year, Sector) %>% 
  summarise(Reported = sum(Reported , na.rm = TRUE),
            IUU = sum(IUU , na.rm = TRUE),
            Discards = sum(Discards , na.rm = TRUE)) %>% 
  mutate(type = "dataset")

trial2<-trial %>% 
  full_join(annual) %>% 
  as.data.frame(trial2)

trial2<-trial2 %>% 
  gather("key", "catch", -Year, -type, -Sector)

ggplot(trial2, aes(x = Year, y = catch, group = key, color = key)) + # color = Sector, group = Sector 
  geom_point()+
  theme_bw()+
  facet_grid(type~Sector)

```

# save final data 

```{r}

final_catch <- catch2

unique(as.data.frame(final_catch)$Year)
fwrite(x = as.data.frame(final_catch), file.path(yannick_dir, "catch_histsoc_1869_2017.csv"))

```

# explore the Unknown_EEZ FGroup in effort

```{r }

# some FGroup == "Unknown_EEZ", what does this mean? 

a<-filter(effort, FGroup == "Unknown_EEZ")
unique(a$Sector) 
unique(a$SAUP)
unique(a$NomActive)
a<-filter(effort, FGroup == "Unknown_EEZ", SAUP == 892)
unique(a$NomActive)
a<-filter(a, NomActive == 28.926300)
a

# is this group on the original files too?  
this_source_path <- file.path("/rd/gem/private/users/yannickr/effort_mapped_bycountry", "mapped_1992_UP_892.csv")
these_data <- read_csv(this_source_path)

filter(these_data, NomActive ==0) # WARNING I don't understand these 0s... 

b<-filter(these_data, FGroup == "Unknown_EEZ", NomActive !=0)
b 
sum(b$NomActive) 
sum(filter(effort, FGroup == "Unknown_EEZ", SAUP == 892, Year ==1992, NomActive !=0)$NomActive) # OK this is matching... 

```

